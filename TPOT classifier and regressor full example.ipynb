{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare algorithms\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "import os \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "os.chdir(\"C:\\\\Users\\\\manka\\\\Documents\\\\GitHub\\\\Machine-learning-quickbook\")\n",
    "\n",
    "ind = ['cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment',\n",
    "       'gill-spacing','gill-size','gill-color','stalk-shape','stalk-root',\n",
    "       'stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring',\n",
    "       'stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type',\n",
    "       'spore-print-color','population','habitat']\n",
    "dep = 'class'\n",
    "\n",
    "names = ind.append(dep)\n",
    "dataset = pd.read_csv(\"mushrooms.csv\")\n",
    "\n",
    "dataset = dataset.sample(500)\n",
    "dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Describe independent and dependent variables\n",
    "\n",
    "def insert_section(n=2):\n",
    "    print('\\n'*n)\n",
    "    print('-----------------------------------------------')\n",
    "\n",
    "def samplesize(dataset, n=1000):\n",
    "    if dataset.shape[0] > n :\n",
    "        sample=n\n",
    "    else :\n",
    "        sample = dataset.shape[0]\n",
    "    return sample\n",
    "\n",
    "def get_label_info(dataset, varlist):\n",
    "    # Get cardinality of each variable \n",
    "    for var in varlist:\n",
    "        print('\\n\\n')\n",
    "        print(\"Number of levels in category '{0}': \\b {1:2.2f} \".format(var, dataset[var].unique().size))\n",
    "        if dataset[var].unique().size < 10 :\n",
    "                print(\"Levels for catgeory '{0}': {1}\".format(var, dataset[var].unique()))\n",
    "\n",
    "\n",
    "def encode_decode_frame(data):\n",
    "    from collections import defaultdict\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    encoder_dict = defaultdict(LabelEncoder)\n",
    "    encoded_data = data.apply(lambda x: encoder_dict[x.name].fit_transform(x))\n",
    "\n",
    "    inverse_transform_lambda = lambda x: encoder_dict[x.name].inverse_transform(x)\n",
    "    labeled_data = encoded_data.apply(inverse_transform_lambda)\n",
    "    \n",
    "    return encoded_data, labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EDA\n",
    "display(print('Display datatypes of the data'))\n",
    "display(dataset.dtypes)\n",
    "\n",
    "display(print('Null profile of the data below'))\n",
    "display(dataset.isnull().sum())\n",
    "\n",
    "display(insert_section())\n",
    "\n",
    "get_label_info(dataset, ind)\n",
    "\n",
    "n = samplesize(dataset,1000)\n",
    "print(n)\n",
    "\n",
    "# report\n",
    "# profile = ProfileReport(dataset.sample(n))\n",
    "# profile.to_file(\"report.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding TPOT methods for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPOT to use for real dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "# define dataset\n",
    "X = dataset[ind]\n",
    "Y = dataset[dep]\n",
    "\n",
    "# Encoding the dataframe's predictors\n",
    "encoded_x, decoded_x = encode_decode_frame(X)\n",
    "\n",
    "# train-test split using encoded X variable\n",
    "train_x, test_x, train_y, test_y = train_test_split( encoded_x.values, Y.values, test_size=0.20, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "\n",
    "# define search\n",
    "model = TPOTClassifier(generations=100, population_size=500, cv=cv\n",
    "                       , scoring='f1', verbosity=2, random_state=1\n",
    "                       , n_jobs=-1, early_stop=2)\n",
    "\n",
    "\n",
    "# perform the search\n",
    "model.fit(train_x, train_y)\n",
    "print(model.score(test_x, test_y))\n",
    "\n",
    "\n",
    "# export the best model\n",
    "model.export('tpot_best_model.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding manual methods of testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-out validation dataset\n",
    "array = dataset.values\n",
    "X = dataset[ind]\n",
    "Y = dataset[dep]\n",
    "train_x, test_x, train_y, test_y = train_test_split( X.values, Y.values, test_size=0.20, random_state=1, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot Check Algorithms (manual process)\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "\tkfold = StratifiedKFold(n_splits=20, random_state=1, shuffle=True)\n",
    "\tcv_results = cross_val_score(model, train_x, train_y, cv=kfold, scoring='accuracy')\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tprint('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n",
    "\n",
    "\n",
    "# Compare Algorithms\n",
    "pyplot.boxplot(results, labels=names)\n",
    "pyplot.title('Algorithm Comparison')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Good way of handling the data \n",
    "import timeit, os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as mpl\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, LabelEncoder, normalize\n",
    "from sklearn.impute import SimpleImputer \n",
    "from tpot import TPOTRegressor, TPOTClassifier\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# Load dataset\n",
    "os.chdir(\"C:\\\\Users\\\\manka\\\\Documents\\\\GitHub\\\\Machine-learning-quickbook\")\n",
    "ind = ['cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment',\n",
    "       'gill-spacing','gill-size','gill-color','stalk-shape','stalk-root',\n",
    "       'stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring',\n",
    "       'stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type',\n",
    "       'spore-print-color','population','habitat']\n",
    "dep = ['class']\n",
    "\n",
    "dataset = pd.read_csv(\"mushrooms.csv\")\n",
    "\n",
    "X = dataset.reindex(columns=[x for x in dataset.columns.values if x != 'class'])        # separate out X\n",
    "y = dataset.reindex(columns=['class'])  # separate out y\n",
    "\n",
    "X = dataset.reindex(columns=ind)        # separate out X\n",
    "y = dataset.reindex(columns=dep)        # separate out y\n",
    "# y = np.ravel(y)                         # flatten the y array\n",
    "\n",
    "# make list of numeric and string columns\n",
    "numeric_cols = [] # could still have ordinal data\n",
    "string_cols = []  # could have ordinal or nominal data\n",
    "\n",
    "for col in X.columns:\n",
    "    if (X.dtypes[col] == np.int64 or X.dtypes[col] == np.int32 or X.dtypes[col] == np.float64):\n",
    "        numeric_cols.append(col)      # True integer or float columns\n",
    "    \n",
    "    if (X.dtypes[col] == np.object):  # Nominal and ordinal columns\n",
    "        string_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with string variables (simplest form of imputation for missing as acreating a new label)\n",
    "X_string = X[string_cols]\n",
    "X_string = X_string.fillna(\"missing\")\n",
    "X_string = X_string.apply(LabelEncoder().fit_transform)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with numeric variables\n",
    "n_imputer = SimpleImputer(missing_values='NaN', copy = True, strategy = 'most_frequent') # imputing with most frequent because some of these numeric columns are ordinal\n",
    "\n",
    "X_numeric = X[numeric_cols]\n",
    "\n",
    "if X_numeric.shape[1] > 0:\n",
    "    X_numeric = n_imputer.fit_transform(X_numeric)\n",
    "    X_numeric = pd.DataFrame(X_numeric, columns = numeric_cols)\n",
    "else:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8124 entries, 0 to 8123\n",
      "Data columns (total 22 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   cap-shape                 8124 non-null   object\n",
      " 1   cap-surface               8124 non-null   object\n",
      " 2   cap-color                 8124 non-null   object\n",
      " 3   bruises                   8124 non-null   object\n",
      " 4   odor                      8124 non-null   object\n",
      " 5   gill-attachment           8124 non-null   object\n",
      " 6   gill-spacing              8124 non-null   object\n",
      " 7   gill-size                 8124 non-null   object\n",
      " 8   gill-color                8124 non-null   object\n",
      " 9   stalk-shape               8124 non-null   object\n",
      " 10  stalk-root                8124 non-null   object\n",
      " 11  stalk-surface-above-ring  8124 non-null   object\n",
      " 12  stalk-surface-below-ring  8124 non-null   object\n",
      " 13  stalk-color-above-ring    8124 non-null   object\n",
      " 14  stalk-color-below-ring    8124 non-null   object\n",
      " 15  veil-type                 8124 non-null   object\n",
      " 16  veil-color                8124 non-null   object\n",
      " 17  ring-number               8124 non-null   object\n",
      " 18  ring-type                 8124 non-null   object\n",
      " 19  spore-print-color         8124 non-null   object\n",
      " 20  population                8124 non-null   object\n",
      " 21  habitat                   8124 non-null   object\n",
      "dtypes: object(22)\n",
      "memory usage: 1.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8124 entries, 0 to 8123\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   class   8124 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 63.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['e', 'p'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['p', 'e', 'e', ..., 'e', 'p', 'e'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[5, 2, 4, ..., 2, 3, 5],\n",
       "       [5, 2, 9, ..., 3, 2, 1],\n",
       "       [0, 2, 8, ..., 3, 2, 3],\n",
       "       ...,\n",
       "       [2, 2, 4, ..., 0, 1, 2],\n",
       "       [3, 3, 4, ..., 7, 4, 2],\n",
       "       [5, 2, 4, ..., 4, 1, 2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(8124, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(8124,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confirm encoded data types\n",
    "display(X.info())\n",
    "display(y.info())\n",
    "\n",
    "# Finalizaing encoced X and Y matrix\n",
    "Y = y\n",
    "yle = LabelEncoder() # Need to ravel to make (1,) matrix\n",
    "y_encoded = yle.fit_transform(np.ravel(Y)) \n",
    "\n",
    "# get the orginal labels bacl\n",
    "display(yle.classes_)\n",
    "display(yle.inverse_transform(y_encoded))\n",
    "\n",
    "X = pd.concat([X_numeric, X_string], axis=1)\n",
    "X_encoded = X.values\n",
    "\n",
    "# View encoded data \n",
    "display(X_encoded)\n",
    "display(y_encoded)\n",
    "display(X_encoded.shape)\n",
    "display(y_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 2, 2, ..., 7, 4, 4],\n",
       "       [5, 3, 9, ..., 1, 5, 0],\n",
       "       [5, 0, 3, ..., 1, 5, 1],\n",
       "       ...,\n",
       "       [2, 3, 3, ..., 1, 5, 4],\n",
       "       [5, 3, 9, ..., 1, 5, 0],\n",
       "       [5, 3, 7, ..., 1, 4, 0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train test splot using only encoded X and Y matrix\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size = .25, random_state = 55)\n",
    "\n",
    "display(X_train)\n",
    "display(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate tpot \n",
    "tpot = TPOTRegressor(verbosity=2,  \n",
    "                    random_state=1, \n",
    "                    scoring='f1',\n",
    "                    periodic_checkpoint_folder=\"intermediate_results\",\n",
    "                    n_jobs=5, \n",
    "                    warm_start = False,\n",
    "                    generations=100, \n",
    "                    population_size=250,\n",
    "                    early_stop=5)\n",
    "times = []\n",
    "scores = []\n",
    "winning_pipes = []\n",
    "\n",
    "# run 2 iterations\n",
    "for x in range(1):\n",
    "    start_time = timeit.default_timer()\n",
    "    tpot.fit(X_train, y_train)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    times.append(elapsed)\n",
    "    winning_pipes.append(tpot.fitted_pipeline_)\n",
    "    scores.append(tpot.score(X_test, y_test))\n",
    "    tpot.export(str(x)+'_tpot_reg.py')\n",
    "\n",
    "# output results\n",
    "times = [time/60 for time in times]\n",
    "print('Times:', times)\n",
    "print('Scores:', scores)   \n",
    "print('Winning pipelines:', winning_pipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate tpot \n",
    "tpot = TPOTClassifier(verbosity=2,\n",
    "                      random_state=1,\n",
    "                      scoring='f1',\n",
    "                      periodic_checkpoint_folder=\"intermediate_results\",\n",
    "                      n_jobs=5,\n",
    "                      warm_start = False,\n",
    "                      generations=100, \n",
    "                      population_size=250,\n",
    "                      early_stop=5)\n",
    "times = []\n",
    "scores = []\n",
    "winning_pipes = []\n",
    "\n",
    "# run 2 iterations\n",
    "for x in range(1):\n",
    "    start_time = timeit.default_timer()\n",
    "    tpot.fit(X_train, y_train)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    times.append(elapsed)\n",
    "    winning_pipes.append(tpot.fitted_pipeline_)\n",
    "    scores.append(tpot.score(X_test, y_test))\n",
    "    tpot.export(str(x)+'_tpot_clf.py')\n",
    "\n",
    "# output results\n",
    "times = [time/60 for time in times]\n",
    "print('Times:', times)\n",
    "print('Scores:', scores)   \n",
    "print('Winning pipelines:', winning_pipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tpot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-07c7c084afc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexported_pipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'random_state'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mexctracted_best_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtpot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitted_pipeline_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mexctracted_best_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tpot' is not defined"
     ]
    }
   ],
   "source": [
    "# Average CV score on the training set was: 1.0\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "exported_pipeline = RandomForestClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.3, min_samples_leaf=13, min_samples_split=19, n_estimators=100)\n",
    "# Fix random state in exported estimator\n",
    "if hasattr(exported_pipeline, 'random_state'):\n",
    "    setattr(exported_pipeline, 'random_state', 1)\n",
    "\n",
    "exctracted_best_model = tpot.fitted_pipeline_.steps[-1][1]\n",
    "exctracted_best_model.feature_importances_\n",
    "\n",
    "# plot feature importance \n",
    "c = sns.color_palette(\"muted\", 3)[2]\n",
    "sns.barplot(x=X.columns.values, y=exctracted_best_model.feature_importances_, color=c)\n",
    "mpl.xticks(rotation=90)\n",
    "mpl.tight_layout()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score :  1.0\n",
      "Balanced Accuracy Score :  1.0\n",
      "Cohen's Kappa Score :  1.0\n",
      "Confusion Matrix Score :  [[0.51255539 0.        ]\n",
      " [0.         0.48744461]]\n",
      "Classification report Score :                precision    recall  f1-score   support\n",
      "\n",
      "           e       1.00      1.00      1.00      1041\n",
      "           p       1.00      1.00      1.00       990\n",
      "\n",
      "    accuracy                           1.00      2031\n",
      "   macro avg       1.00      1.00      1.00      2031\n",
      "weighted avg       1.00      1.00      1.00      2031\n",
      "\n",
      "Hammings Loss Score :  0.0\n",
      "Average Precision Score :  1.0\n",
      "F1 Score :  1.0\n",
      "F-beta Score :  1.0\n",
      "Precision Recall Fscore Support Score :  (1.0, 1.0, 1.0, None)\n",
      "Recall Score :  1.0\n",
      "Precision Score :  1.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tpot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-39a25b23ee27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# adding ROC curves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mprobas_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtpot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mprobas_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtpot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tpot' is not defined"
     ]
    }
   ],
   "source": [
    "# Get all sorts of classification score matrices\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, cohen_kappa_score, \\\n",
    "    confusion_matrix, classification_report, hamming_loss, average_precision_score, \\\n",
    "    f1_score, fbeta_score, precision_recall_fscore_support,recall_score, precision_score, \\\n",
    "    precision_recall_curve, roc_auc_score, roc_curve, auc\n",
    "\n",
    "exported_pipeline.fit(X_train, y_train)\n",
    "results = exported_pipeline.predict(X_test)\n",
    "\n",
    "prediction = exported_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Accuracy Score : \" , accuracy_score(y_test, prediction))\n",
    "print(\"Balanced Accuracy Score : \" , balanced_accuracy_score(y_test, prediction))\n",
    "print(\"Cohen's Kappa Score : \" , cohen_kappa_score(y_test, prediction))\n",
    "print(\"Confusion Matrix Score : \" , confusion_matrix(y_test, prediction, normalize='all'))\n",
    "print(\"Classification report Score : \" , classification_report(y_test, prediction, target_names=yle.classes_))\n",
    "print(\"Hammings Loss Score : \" , hamming_loss(y_test, prediction))\n",
    "print(\"Average Precision Score : \" , average_precision_score(y_test, prediction))\n",
    "print(\"F1 Score : \" , f1_score(y_test, prediction))\n",
    "print(\"F-beta Score : \" , fbeta_score(y_test, prediction, beta = 0)) # beta < 0 precision weight;  beta > 0 recall weight :  max(abs(beta)) == 1 \n",
    "print(\"Precision Recall Fscore Support Score : \" , precision_recall_fscore_support(y_test, prediction, beta = 0, average = 'micro')) # average [None (default), ‘binary’, ‘micro’, ‘macro’, ‘samples’, ‘weighted’]\n",
    "print(\"Recall Score : \" , recall_score(y_test, prediction))\n",
    "print(\"Precision Score : \" , precision_score(y_test, prediction))\n",
    "\n",
    "# adding ROC curves \n",
    "try:\n",
    "    probas_ = tpot.predict_proba(X_test)[:, 1]\n",
    "except AttributeError:\n",
    "    probas_ = tpot.decision_function(X_test)\n",
    "\n",
    "print(roc_auc_score(y_test, probas_))\n",
    "print(\"Precision Score : \", precision_recall_curve(y_test, probas_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-89fff841f504>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mroc_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mfpr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mroc_auc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "n_classes = 1\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), probas_.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exported_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6093,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
