{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare algorithms\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as mpl\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "import os, timeit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, LabelEncoder, normalize\n",
    "from sklearn.impute import SimpleImputer \n",
    "from tpot import TPOTRegressor, TPOTClassifier\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "# Describe independent and dependent variables\n",
    "\n",
    "def insert_section(n=2):\n",
    "    print('\\n'*n)\n",
    "    print('-----------------------------------------------')\n",
    "\n",
    "def samplesize(dataset, n=1000):\n",
    "    if dataset.shape[0] > n :\n",
    "        sample=n\n",
    "    else :\n",
    "        sample = dataset.shape[0]\n",
    "    return sample\n",
    "\n",
    "def get_label_info(dataset, varlist):\n",
    "    # Get cardinality of each variable \n",
    "    for var in varlist:\n",
    "        print('\\n\\n')\n",
    "        print(\"Number of levels in category '{0}': \\b {1:2.2f} \".format(var, dataset[var].unique().size))\n",
    "        if dataset[var].unique().size < 10 :\n",
    "                print(\"Levels for catgeory '{0}': {1}\".format(var, dataset[var].unique()))\n",
    "\n",
    "\n",
    "def encode_decode_frame(data):\n",
    "    from collections import defaultdict\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    encoder_dict = defaultdict(LabelEncoder)\n",
    "    encoded_data = data.apply(lambda x: encoder_dict[x.name].fit_transform(x))\n",
    "\n",
    "    inverse_transform_lambda = lambda x: encoder_dict[x.name].inverse_transform(x)\n",
    "    labeled_data = encoded_data.apply(inverse_transform_lambda)\n",
    "    \n",
    "    return encoded_data, labeled_data\n",
    "\n",
    "\n",
    "class MultiColumnLabelEncoder:\n",
    "    \n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.encoders = {}\n",
    "        columns = X.columns if self.columns is None else self.columns\n",
    "        for col in columns:\n",
    "            self.encoders[col] = LabelEncoder().fit(X[col])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        output = X.copy()\n",
    "        columns = X.columns if self.columns is None else self.columns\n",
    "        for col in columns:\n",
    "            output[col] = self.encoders[col].transform(X[col])\n",
    "        return output\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X,y).transform(X)\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        output = X.copy()\n",
    "        columns = X.columns if self.columns is None else self.columns\n",
    "        for col in columns:\n",
    "            output[col] = self.encoders[col].inverse_transform(X[col])\n",
    "        return output\n",
    "\n",
    "\n",
    "def num_str_cols(df):\n",
    "    numeric_cols = [] # could still have ordinal data\n",
    "    string_cols = []  # could have ordinal or nominal data\n",
    "    for col in df.columns:\n",
    "        if (df.dtypes[col] == np.int64 or df.dtypes[col] == np.int32 or df.dtypes[col] == np.float64):\n",
    "            numeric_cols.append(col)      # True integer or float columns\n",
    "\n",
    "        if (df.dtypes[col] == np.object):  # Nominal and ordinal columns\n",
    "            string_cols.append(col)\n",
    "    return numeric_cols, string_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = pd.read_csv(\"mushrooms.csv\")\n",
    "os.chdir(\"C:\\\\Users\\\\manka\\\\Documents\\\\GitHub\\\\Machine-learning-quickbook\")\n",
    "ind = ['cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment',\n",
    "       'gill-spacing','gill-size','gill-color','stalk-shape','stalk-root',\n",
    "       'stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring',\n",
    "       'stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type',\n",
    "       'spore-print-color','population','habitat']\n",
    "dep = ['class']\n",
    "\n",
    "# EDA\n",
    "display(print('Display datatypes of the data'))\n",
    "display(dataset.dtypes)\n",
    "\n",
    "display(print('Null profile of the data below'))\n",
    "display(dataset.isnull().sum())\n",
    "\n",
    "display(insert_section())\n",
    "\n",
    "get_label_info(dataset, ind)\n",
    "\n",
    "n = samplesize(dataset,1000)\n",
    "print(n)\n",
    "\n",
    "# report\n",
    "# profile = ProfileReport(dataset.sample(n))\n",
    "# profile.to_file(\"report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select X matrix as all but one as outcome variable\n",
    "# X = dataset.reindex(columns=[x for x in dataset.columns.values if x != 'class'])        # separate out X\n",
    "X = dataset.reindex(columns=ind)        # separate out X\n",
    "y = dataset.reindex(columns=dep)        # separate out y\n",
    "\n",
    "\n",
    "# Fixing X matrix \n",
    "# Dealing with string variables (simplest form of imputation for missing as acreating a new label)\n",
    "X_string = X[num_str_cols(X)[1]]\n",
    "X_string = X_string.fillna(\"missing\")\n",
    "\n",
    "# Dealing with numeric variables\n",
    "n_imputer = SimpleImputer(missing_values='NaN', copy = True, strategy = 'most_frequent') # imputing with most frequent because some of these numeric columns are ordinal\n",
    "X_numeric = X[num_str_cols(X)[0]]\n",
    "if X_numeric.shape[1] > 0:\n",
    "    X_numeric = n_imputer.fit_transform(X_numeric)\n",
    "    X_numeric = pd.DataFrame(X_numeric, columns = numeric_cols)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "X_string = X_string.apply(LabelEncoder().fit_transform) \n",
    "X = pd.concat([X_numeric, X_string], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the data for further use \n",
    "display(X.info())\n",
    "display(y.info())\n",
    "\n",
    "# Finalizaing encoced X and Y matrix\n",
    "Y = y\n",
    "yle = LabelEncoder() # Need to ravel to make (1,) matrix\n",
    "yle.fit(Y)\n",
    "y_encoded = yle.transform(Y)\n",
    "\n",
    "# get the orginal labels bacl\n",
    "# display(yle.classes_)\n",
    "# display(yle.inverse_transform(y_encoded))\n",
    "\n",
    "xle = MultiColumnLabelEncoder()\n",
    "xle.fit(X)\n",
    "X_encoded = xle.transform(X)\n",
    "\n",
    "# View encoded data \n",
    "display(X_encoded)\n",
    "display(y_encoded)\n",
    "display(X_encoded.shape)\n",
    "display(y_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test splot using only encoded X and Y matrix\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size = .25, random_state = 55)\n",
    "\n",
    "display(X_train)\n",
    "display(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate tpot \n",
    "tpot = TPOTRegressor(verbosity=2,  \n",
    "                    random_state=1, \n",
    "                    scoring='f1',\n",
    "                    periodic_checkpoint_folder=\"intermediate_results\",\n",
    "                    n_jobs=5, \n",
    "                    warm_start = False,\n",
    "                    generations=100, \n",
    "                    population_size=250,\n",
    "                    early_stop=5)\n",
    "times = []\n",
    "scores = []\n",
    "winning_pipes = []\n",
    "\n",
    "# run 2 iterations\n",
    "for x in range(1):\n",
    "    start_time = timeit.default_timer()\n",
    "    tpot.fit(X_train, y_train)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    times.append(elapsed)\n",
    "    winning_pipes.append(tpot.fitted_pipeline_)\n",
    "    scores.append(tpot.score(X_test, y_test))\n",
    "    tpot.export(str(x)+'_tpot_reg.py')\n",
    "\n",
    "# output results\n",
    "times = [time/60 for time in times]\n",
    "print('Times:', times)\n",
    "print('Scores:', scores)   \n",
    "print('Winning pipelines:', winning_pipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate tpot \n",
    "tpot = TPOTClassifier(verbosity=2,\n",
    "                      random_state=1,\n",
    "                      scoring='f1',\n",
    "                      periodic_checkpoint_folder=\"intermediate_results\",\n",
    "                      n_jobs=5,\n",
    "                      warm_start = False,\n",
    "                      generations=100, \n",
    "                      population_size=250,\n",
    "                      early_stop=5)\n",
    "times = []\n",
    "scores = []\n",
    "winning_pipes = []\n",
    "\n",
    "# run 2 iterations\n",
    "for x in range(1):\n",
    "    start_time = timeit.default_timer()\n",
    "    tpot.fit(X_train, y_train)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    times.append(elapsed)\n",
    "    winning_pipes.append(tpot.fitted_pipeline_)\n",
    "    scores.append(tpot.score(X_test, y_test))\n",
    "    tpot.export(str(x)+'_tpot_clf.py')\n",
    "\n",
    "# output results\n",
    "times = [time/60 for time in times]\n",
    "print('Times:', times)\n",
    "print('Scores:', scores)   \n",
    "print('Winning pipelines:', winning_pipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average CV score on the training set was: 1.0\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "exported_pipeline = RandomForestClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.3, min_samples_leaf=13, min_samples_split=19, n_estimators=100)\n",
    "# Fix random state in exported estimator\n",
    "if hasattr(exported_pipeline, 'random_state'):\n",
    "    setattr(exported_pipeline, 'random_state', 1)\n",
    "\n",
    "exctracted_best_model = tpot.fitted_pipeline_.steps[-1][1]\n",
    "exctracted_best_model.feature_importances_\n",
    "\n",
    "# plot feature importance \n",
    "c = sns.color_palette(\"muted\", 3)[2]\n",
    "sns.barplot(x=X.columns.values, y=exctracted_best_model.feature_importances_, color=c)\n",
    "mpl.xticks(rotation=90)\n",
    "mpl.tight_layout()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all sorts of classification score matrices\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, cohen_kappa_score, \\\n",
    "    confusion_matrix, classification_report, hamming_loss, average_precision_score, \\\n",
    "    f1_score, fbeta_score, precision_recall_fscore_support,recall_score, precision_score, \\\n",
    "    precision_recall_curve, roc_auc_score, roc_curve, auc\n",
    "\n",
    "exported_pipeline.fit(X_train, y_train)\n",
    "results = exported_pipeline.predict(X_test)\n",
    "\n",
    "prediction = exported_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Accuracy Score : \" , accuracy_score(y_test, prediction))\n",
    "print(\"Balanced Accuracy Score : \" , balanced_accuracy_score(y_test, prediction))\n",
    "print(\"Cohen's Kappa Score : \" , cohen_kappa_score(y_test, prediction))\n",
    "print(\"Confusion Matrix Score : \" , confusion_matrix(y_test, prediction, normalize='all'))\n",
    "print(\"Classification report Score : \" , classification_report(y_test, prediction, target_names=yle.classes_))\n",
    "print(\"Hammings Loss Score : \" , hamming_loss(y_test, prediction))\n",
    "print(\"Average Precision Score : \" , average_precision_score(y_test, prediction))\n",
    "print(\"F1 Score : \" , f1_score(y_test, prediction))\n",
    "print(\"F-beta Score : \" , fbeta_score(y_test, prediction, beta = 0)) # beta < 0 precision weight;  beta > 0 recall weight :  max(abs(beta)) == 1 \n",
    "print(\"Precision Recall Fscore Support Score : \" , precision_recall_fscore_support(y_test, prediction, beta = 0, average = 'micro')) # average [None (default), ‘binary’, ‘micro’, ‘macro’, ‘samples’, ‘weighted’]\n",
    "print(\"Recall Score : \" , recall_score(y_test, prediction))\n",
    "print(\"Precision Score : \" , precision_score(y_test, prediction))\n",
    "\n",
    "# adding ROC curves \n",
    "try:\n",
    "    probas_ = tpot.predict_proba(X_test)[:, 1]\n",
    "except AttributeError:\n",
    "    probas_ = tpot.decision_function(X_test)\n",
    "\n",
    "print(roc_auc_score(y_test, probas_))\n",
    "print(\"Precision Score : \", precision_recall_curve(y_test, probas_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "n_classes = 1\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), probas_.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exported_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_str_cols(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[num_str_cols(X)[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
